import re
import json
from typing import Any, List, Dict, Optional

class GuessNumAgent():
    """
    Agent that uses an injected LLM provider for feedback-driven iterative guessing.
    Now protocol-aware: all communications follow the communication protocol.
    
    In the new game mode, agents:
    - Receive per-step feedback on whether their guess is correct
    - Communicate their guess history and feedback to other agents via protocol messages
    - Use LLM to reason about next guess based on feedback patterns
    """

    def __init__(self, agent_id: str, provider, protocol: str, initial_guess: int = 0, num_choices: int = 10):
        self.agent_id = agent_id
        self.provider = provider
        self.protocol = protocol
        self.initial_guess = initial_guess
        self.current_guess = initial_guess
        self.num_choices = num_choices
        self.num_agents: Optional[int] = None
        self.current_step: int = 0
        
        # Track game state
        self.received_messages: List[Dict[str, Any]] = []
        self.last_observation: Optional[Dict[str, Any]] = None
        self.is_correct: bool = False
        self.guess_history: List[int] = [self.initial_guess]
        self.feedback_history: List[bool] = []  # True if correct, False if incorrect
        
        # LLM interaction tracking
        self.last_prompt: Optional[str] = None
        self.last_llm_output: Optional[str] = None
        self._last_message: Optional[Dict[str, Any]] = None  # Message generated by LLM in send_message()

    def reset(self) -> None:
        self.current_guess = self.initial_guess
        self.guess_history = [self.initial_guess]
        self.feedback_history = []
        self.received_messages = []
        self.is_correct = False
        self.current_step = 0
        self._last_message = None
        self.last_prompt = None
        self.last_llm_output = None

    def update_observation(self, observation: Dict[str, Any]) -> None:
        """Update agent state based on environment feedback."""
        self.last_observation = observation
        if "is_correct" in observation:
            self.is_correct = observation["is_correct"]
            self.feedback_history.append(self.is_correct)

    def receive_messages(self, messages: Dict[str, Any]) -> None:
        """Receive protocol-validated messages from other agents."""
        self.received_messages.append(messages)

    def send_message(self) -> Dict[str, Any]:
        """
        Generate a message using LLM that follows the communication protocol.
        This message includes the agent's current state and reasoning.
        
        The LLM generates the complete message based on the protocol,
        which can include state information, reasoning, etc.
        """
        # Build prompt with protocol file content
        prompt = self._build_protocol_aware_prompt()
        self.last_prompt = prompt
        # print(f"[Agent {self.agent_id}] LLM Prompt:\n{prompt}\n")

        # Call LLM provider
        llm_output = self.provider.call(prompt)
        self.last_llm_output = llm_output
        # print(f"[Agent {self.agent_id}] LLM Output:\n{llm_output}\n")
        
        # Parse JSON message from LLM output
        message = self._parse_llm_message(llm_output)
        self._last_message = message  # Store for later use in act()
        
        return message

    def act(self) -> int:
        """
        Decide next action based on environment feedback.
        
        Extracts the answer from the message previously generated by LLM in send_message().
        """
        self.current_step += 1

        # If already correct, stay with current guess
        # if self.is_correct:
        #     return self.current_guess

        # Extract answer from the LLM-generated message
        if self._last_message is not None:
            new_guess = self._last_message.get('next_guess')
            
            if new_guess is not None and isinstance(new_guess, int):
                self.current_guess = new_guess
                self.guess_history.append(new_guess)

        return self.current_guess

    def _parse_llm_message(self, llm_output: str) -> Dict[str, Any]:
        """
        Parse JSON message from LLM output.
        Strips any 'think' or reasoning content and keeps only the final JSON.
        """
        cleaned_output = llm_output.strip()

        # 1 Remove <think>...</think> blocks (DeepSeek / Qwen style)
        cleaned_output = re.sub(
            r"<think>.*?</think>",
            "",
            cleaned_output,
            flags=re.DOTALL | re.IGNORECASE,
        ).strip()

        # 2 Remove common reasoning prefixes (OpenAI / Claude style)
        reasoning_markers = [
            "Thoughts:",
            "Reasoning:",
            "Analysis:",
            "Chain of thought:",
            "Final Answer:",
        ]
        for marker in reasoning_markers:
            if marker in cleaned_output:
                cleaned_output = cleaned_output.split(marker, 1)[-1].strip()

        # 3 Remove Markdown code blocks (```json ... ```)
        # 移除开头的 ```json 或 ```JSON
        cleaned_output = re.sub(r'^```(?:json)?\s*', '', cleaned_output, flags=re.IGNORECASE)
        # 移除结尾的 ```
        cleaned_output = re.sub(r'\s*```$', '', cleaned_output)

        # 4 Try direct JSON parsing
        try:
            return json.loads(cleaned_output)
        except json.JSONDecodeError:
            pass

        # 4 Fallback: extract the last JSON object in the text
        matches = list(re.finditer(r"\{.*?\}", cleaned_output, flags=re.DOTALL))
        if matches:
            try:
                return json.loads(matches[-1].group())
            except json.JSONDecodeError:
                pass

        # 5 Fail loudly (important for debugging RL / protocol issues)
        raise ValueError(
            "Could not parse final JSON message from LLM output after removing reasoning.\n"
            f"Original output:\n{llm_output}"
            f"\nCleaned output:\n{cleaned_output}"
        )

    def _build_protocol_aware_prompt(self) -> str:
        """
        Build a prompt with the protocol file content.
        Very simple: just read the file and insert it into the prompt.
        """
        lines = [
            f"You are {self.agent_id} playing a collaborative guessing game with other agents.",
            f"The number to guess is an integer within a specified range (0 -> {self.num_choices - 1}).",
            "Only one number is correct in each game.",
            "",
            "===== COMMUNICATION PROTOCOL =====",
            self.protocol,
            "===== END PROTOCOL =====",
            "",
            f"Current state:",
            f"  - Agent ID: {self.agent_id}",
            f"  - My guess: {self.current_guess}",
            f"  - Step: {self.current_step}",
            f"  - Correct: {self.is_correct}",
            f"  - Guess history: {self.guess_history}",
            f"  - Feedback history: {self.feedback_history}",
        ]
        
        # Add other agents' messages if any
        if self.received_messages:
            lines.append("")
            lines.append("Other agents' recent messages:")
            for msg in self.received_messages[-3:]:
                lines.append(f"  {json.dumps(msg)}")
        
        lines.extend([
            "",
            "===== COLLABORATIVE REASONING RULES =====",
            "You MUST consider other agents' feedbacks and your current state.",
            "",
            "Rules you MUST follow:",
            "1. If no one has found the correct number, choose a new guess that has not been guessed by any one.",
            "2. If anyone has found the correct number, choose it as your next answer."
        ])

        lines.extend([
            "",
            "===== OUTPUT RULES =====",
            "Following the protocol, return ONLY the JSON message without any other text or label (```json or ```):",
            "Your JSON message MUST be correct in JSON grammar.",
            "Include the 'next_guess' field with the correct number.",
        ])
        
        return "\n".join(lines)
